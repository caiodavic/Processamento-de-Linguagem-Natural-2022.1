{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdepijmAGtsIF4PTCz2KW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caiodavic/Processamento-de-Linguagem-Natural-2022.1/blob/main/Agrupamento_com_k_means_PLN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKaLArVhicaW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leitura de Dados\n",
        "- Vamos utilizar sátiras políticas do sensacionalista."
      ],
      "metadata": {
        "id": "OigjsYsOldB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/caiolibanio/atividade_NLP/master/csv_satiras_politicas.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "pN2KkfgKlk_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
        "stop_words = stopwords.words(\"portuguese\")\n",
        "stop_words.extend(['nao','diz'])\n",
        "def pre_process(sentence):\n",
        "  \n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  filtered = ' '.join([i for i in tokens if i not in stop_words])\n",
        "  return filtered"
      ],
      "metadata": {
        "id": "Klx0Ol90fnKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = df['title'].apply(pre_process)\n",
        "texts = df['text'].apply(pre_process)\n",
        "small_df = pd.DataFrame({'title':titles,'texts':texts})"
      ],
      "metadata": {
        "id": "v1pdlQYkvG2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extração de Features - TF-IDF\n",
        "- Função para gerar as matrizes TF-IDF a partir da coleção de documentos textuais passados como entrada"
      ],
      "metadata": {
        "id": "NL8aZQr8u-L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buildTFIDFVectorizer(docs):\n",
        "  vect = TfidfVectorizer()\n",
        "  matrix = vect.fit_transform(docs)\n",
        "  return matrix, vect"
      ],
      "metadata": {
        "id": "zh0dVH_jvwFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, v = buildTFIDFVectorizer(texts)\n",
        "tf_idf = pd.DataFrame(data = m.toarray(), columns = v.get_feature_names_out())"
      ],
      "metadata": {
        "id": "mYaoP056waPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "fig = plt.figure(figsize= (30,10))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "pca = PCA(n_components=2)\n",
        "scatter_plot_points = pca.fit_transform(m.toarray())\n",
        "\n",
        "ax.scatter(scatter_plot_points[:, 0], scatter_plot_points[:,1])"
      ],
      "metadata": {
        "id": "iTjoGZ4c1or5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executando o k-means\n",
        "- Vamos executar o k-means com o número k já definido, k=3."
      ],
      "metadata": {
        "id": "5R9mShVk-9FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "88c0sKN0_DBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(m)\n",
        "groups = kmeans.predict(m)\n",
        "groups"
      ],
      "metadata": {
        "id": "cDN9V_ATG0qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_df['group'] = groups"
      ],
      "metadata": {
        "id": "Cv8WfpfQLCJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(m):\n",
        "  fig = plt.figure(figsize= (30,10))\n",
        "  ax = fig.add_subplot(1, 2, 1)\n",
        "  pca = PCA(n_components=3, random_state= 0)\n",
        "  reduced_features = pca.fit_transform(m.toarray())\n",
        "  reduced_cluster_centers = pca.transform(kmeans.cluster_centers_)\n",
        "  ax.scatter(reduced_features[:,0], reduced_features[:,1], c=kmeans.predict(m))\n",
        "  ax.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Zf5EpwlXIViE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(m)"
      ],
      "metadata": {
        "id": "Too6oZGkJxCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisando as palavras mais frequentes de cada grupo\n",
        "- A partir de agora vamos analisar com um histograma e um wordCloud as palavras mais frequentes de cada cluster"
      ],
      "metadata": {
        "id": "m6YSiTX1UjyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def counting_words(df,predict):\n",
        "  labels = np.unique(predict)\n",
        "  couting = []\n",
        "  for i in labels:\n",
        "    mask = df['group'] == i\n",
        "    next_to_cluster = df[mask]\n",
        "    a = next_to_cluster['title'].str.split()\n",
        "    freq_word = []\n",
        "    for i in a:\n",
        "      freq_word = freq_word + i    \n",
        "    couting.append(nltk.FreqDist(freq_word))\n",
        "\n",
        "  return couting  "
      ],
      "metadata": {
        "id": "86Ut778yWtVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "def plot_horizontal(counting):\n",
        "  \n",
        "  for i in range(len(counting)):\n",
        "    y =[j[0] for j in counting[i].most_common(5)] \n",
        "    x=[j[1] for j in counting[i].most_common(5)] \n",
        "\n",
        "    \n",
        "    plt.barh(y, x) \n",
        "    plt.ylabel(\"words\") \n",
        "    plt.xlabel(\"frequency\")  \n",
        "    plt.title(f'5 most frequent words from cluster {i}') \n",
        "    plt.show() "
      ],
      "metadata": {
        "id": "WlRPXBnnqPY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "couting_aux = counting_words(small_df,groups)"
      ],
      "metadata": {
        "id": "BrYvwIueiy7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_horizontal(couting_aux)"
      ],
      "metadata": {
        "id": "SXATBlEtrVRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "Ey_6DHpBUwZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_word_clouds(centroids):\n",
        "    wordcloud = WordCloud(max_font_size=100, background_color = 'white')\n",
        "    for i in range(0, len(centroids)):\n",
        "        centroid_dict = centroids[i]        \n",
        "        wordcloud.generate_from_frequencies(centroid_dict)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.title(f'Cluster {i}')\n",
        "        plt.imshow(wordcloud)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "C4neIQ-RjLG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_word_clouds(counting_words(small_df,groups))"
      ],
      "metadata": {
        "id": "7__b9GjAjdIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Escolhendo o K no K-Means\n",
        "- Precisamos de alguma heurística que nos ajude a escolher o K com um pouco mais de critério.\n",
        "- Vamos utilizar o método do Cotovelo.\n",
        "\n",
        "## O método do cotovelo\n",
        "\n",
        "O método mais comum é usar uma métrica de heterogeneidade dentro dos clusters. Com uma métrica desse tipo, uma solução com k maior sempre será mais homogênea. O método então consiste de examinar vários valores de $k$ e *escolher um valor a partir do qual* **não há ganho substancial em aumentar k**.\n",
        "\n",
        "Uma medida de heterogeneidade dos grupos é o SSD: \n",
        "\n",
        "SSD = $\\sum{(x_i - c_i)^2}$\n",
        "\n",
        "Professor Nazareno Andrade"
      ],
      "metadata": {
        "id": "242N-PRUj_PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "JKfYz_WfoIms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quality = pd.DataFrame(columns = ['k', 'ssd'])\n",
        "for k in tqdm(range(1,5)):\n",
        "  kmeans = KMeans(n_clusters=k, n_init=20)\n",
        "  clsuter_labels = kmeans.fit_predict(m)\n",
        "  quality = quality.append({'k': k, 'ssd' : kmeans.inertia_}, ignore_index=True)\n",
        "\n",
        "quality.head(10)"
      ],
      "metadata": {
        "id": "oRoMY9jDlsHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}